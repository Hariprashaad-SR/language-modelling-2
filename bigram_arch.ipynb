{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Bigram Model**\n",
        "\n",
        "Here we will generate sequences of words that mimic Shakespearean language using only the preceding text as context. This model predicts the next word based on the previous word, creating text that captures the style and flow of Shakespeare's writing. By training on Shakespeare's works, the model learns the probabilistic relationships between words, allowing it to generate coherent and stylistically accurate sentences. But since it only does it using a single word, the performance might not be optimal"
      ],
      "metadata": {
        "id": "DRG04pbfN9hl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "Gw2vZGLsdZ3w"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Length of the dataset : {len(text)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mh6jufnidlj1",
        "outputId": "1f0a9891-fc24-4773-89eb-c9a9ecec3a54"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of the dataset : 1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnvHTeJKdsgK",
        "outputId": "a96448a9-caab-4b35-9413-30cd2fee83f2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(f'Characters : {\"\".join(chars)}\\nVocab size : {vocab_size}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDQ--NpmeTQb",
        "outputId": "8d3eaa9f-3ce6-42f2-fa9f-3378aae1c9ed"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Characters : \n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "Vocab size : 65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stoi = {ch : i for i, ch in enumerate(chars)}\n",
        "itos = {i : ch for i, ch in enumerate(chars)}\n",
        "encode = lambda s : [stoi[c] for c in s]\n",
        "decode = lambda l : \"\".join([itos[i] for i in l])"
      ],
      "metadata": {
        "id": "RvfwicIEfDix"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encode(\"Hariprashaad\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6d6QfQEFgAwx",
        "outputId": "5492fb13-71a2-4b2f-8daf-da89124f9bc4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[20, 39, 56, 47, 54, 56, 39, 57, 46, 39, 39, 42]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decode([20, 39, 56, 47, 54, 56, 39, 57, 46, 39, 39, 42])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "SCug_f9MglO7",
        "outputId": "53c73fda-9815-4619-9b1b-c186be093909"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hariprashaad'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Modelling**"
      ],
      "metadata": {
        "id": "-gCQcQ6wgvO0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "Qx_4h3xRgp-s"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = torch.tensor(encode(text), dtype = torch.long)\n",
        "print(data.shape, data.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipvzeXQLgzHu",
        "outputId": "dd8d7951-b012-4795-a7f5-d5b704991c17"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394]) torch.int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data[:100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVvUrKFDhBD8",
        "outputId": "9d115e0f-4961-4b4e-9a14-8b26c5d96597"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
              "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
              "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
              "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
              "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
              "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **(i) Train-test split**"
      ],
      "metadata": {
        "id": "c_Z710gbhzV-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "wOPzIeCmhjgZ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8\n",
        "train_data[:block_size +1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHgtxdS7h3Cw",
        "outputId": "27adf291-9d72-455b-946e-ce3d5237c0ee"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = train_data[:block_size]\n",
        "y = train_data[1 : block_size +1]\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f\"when input is {context} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z60yqfKKi44s",
        "outputId": "b36719fb-9a78-4318-8c45-6e93c51a9487"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([18]) the target: 47\n",
            "when input is tensor([18, 47]) the target: 56\n",
            "when input is tensor([18, 47, 56]) the target: 57\n",
            "when input is tensor([18, 47, 56, 57]) the target: 58\n",
            "when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
            "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hence our transformer could predict from any number of words from 1 to block_size"
      ],
      "metadata": {
        "id": "fKFRoIomjql6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "batch_size = 4\n",
        "block_size = 8\n",
        "\n",
        "def get_batch(split):\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "  x = torch.stack([data[i: i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i + 1 : i + block_size + 1] for i in ix])\n",
        "  return x,y"
      ],
      "metadata": {
        "id": "SrtGrvgJjmyV"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xb, yb = get_batch('train')\n",
        "print('inputs : ', xb.shape)\n",
        "print('targets : ', yb.shape)\n",
        "\n",
        "print('------')\n",
        "for b in range(batch_size):\n",
        "  for t in range(block_size):\n",
        "    context = xb[b, : t+1]\n",
        "    target = yb[b, t]\n",
        "    print(f\"when input is {context.tolist()} the target: {itos[target.item()]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GcwzseYCmRNT",
        "outputId": "b7941f5d-0eab-4ce2-f3be-1ff2578bfb47"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs :  torch.Size([4, 8])\n",
            "targets :  torch.Size([4, 8])\n",
            "------\n",
            "when input is [24] the target: e\n",
            "when input is [24, 43] the target: t\n",
            "when input is [24, 43, 58] the target: '\n",
            "when input is [24, 43, 58, 5] the target: s\n",
            "when input is [24, 43, 58, 5, 57] the target:  \n",
            "when input is [24, 43, 58, 5, 57, 1] the target: h\n",
            "when input is [24, 43, 58, 5, 57, 1, 46] the target: e\n",
            "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target: a\n",
            "when input is [44] the target: o\n",
            "when input is [44, 53] the target: r\n",
            "when input is [44, 53, 56] the target:  \n",
            "when input is [44, 53, 56, 1] the target: t\n",
            "when input is [44, 53, 56, 1, 58] the target: h\n",
            "when input is [44, 53, 56, 1, 58, 46] the target: a\n",
            "when input is [44, 53, 56, 1, 58, 46, 39] the target: t\n",
            "when input is [44, 53, 56, 1, 58, 46, 39, 58] the target:  \n",
            "when input is [52] the target: t\n",
            "when input is [52, 58] the target:  \n",
            "when input is [52, 58, 1] the target: t\n",
            "when input is [52, 58, 1, 58] the target: h\n",
            "when input is [52, 58, 1, 58, 46] the target: a\n",
            "when input is [52, 58, 1, 58, 46, 39] the target: t\n",
            "when input is [52, 58, 1, 58, 46, 39, 58] the target:  \n",
            "when input is [52, 58, 1, 58, 46, 39, 58, 1] the target: h\n",
            "when input is [25] the target: E\n",
            "when input is [25, 17] the target: O\n",
            "when input is [25, 17, 27] the target: :\n",
            "when input is [25, 17, 27, 10] the target: \n",
            "\n",
            "when input is [25, 17, 27, 10, 0] the target: I\n",
            "when input is [25, 17, 27, 10, 0, 21] the target:  \n",
            "when input is [25, 17, 27, 10, 0, 21, 1] the target: p\n",
            "when input is [25, 17, 27, 10, 0, 21, 1, 54] the target: a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **(ii) Bigram modelling**"
      ],
      "metadata": {
        "id": "PU4t2YCer3Gy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets = None):\n",
        "    logits = self.token_embedding_table(idx) #(B, T, C)\n",
        "\n",
        "    if targets == None:\n",
        "      loss = None\n",
        "    else:\n",
        "      # Since cross-entropy expects it to be either (X, C) or (X, C, Y)\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B * T, C)\n",
        "      targets = targets.view(B * T)\n",
        "\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "    return (loss, logits)\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    # idx is (B, T) array of current context\n",
        "    for _ in range(max_new_tokens):\n",
        "      # get the prediction with the current idx\n",
        "      loss, logits = self(idx)\n",
        "      # focus only on the last time step\n",
        "      logits = logits[:, -1, :]\n",
        "      # apply softmax\n",
        "      probs = F.softmax(logits, dim = -1)\n",
        "      # generate the next index using multinomial\n",
        "      idx_next = torch.multinomial(probs, num_samples = 1)\n",
        "      # concat the idx_next with idx to generate new idx\n",
        "      idx = torch.cat((idx, idx_next), dim = 1)\n",
        "    return idx"
      ],
      "metadata": {
        "id": "KrZkKmMUr56f"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = BigramLanguageModel(vocab_size)\n",
        "loss, logits = n(xb, yb) # (4, 8)\n",
        "print(logits.shape) # (4, 8, 65)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJa5N8WHuf4q",
        "outputId": "d8def3d5-5aef-4db3-8a76-3d2f4fb27300"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 65])\n",
            "tensor(4.8786, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "indices = n.generate(idx = torch.zeros((1,1), dtype = torch.long), max_new_tokens = 100)\n",
        "indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPC9KM1l4Zdo",
        "outputId": "aee7de80-3096-48e1-c1bf-8cfbb56d8728"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
              "         22, 45, 27, 24, 34, 64,  5, 30, 21, 53, 16, 55, 20, 42, 46, 57, 34,  4,\n",
              "         60, 24, 24, 62, 39, 58, 48, 57, 41, 25, 54, 61, 24, 17, 30, 31, 28, 63,\n",
              "         39, 53,  8, 55, 44, 64, 57,  3, 37, 57,  3, 64, 18,  7, 61,  6, 11, 43,\n",
              "         17, 49, 64, 62, 48, 45, 15, 23, 18, 15, 46, 57,  2, 47, 35, 35,  8, 27,\n",
              "         40, 64, 16, 52, 62, 13,  1, 25, 57,  3,  9]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(indices[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZW3-8iiG7Dyw",
        "outputId": "6428abb4-ba7a-457c-c84c-312a947c9c04"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **(iii) Creating a pytorch optimizer**"
      ],
      "metadata": {
        "id": "hzHQwLJP74fV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(n.parameters(), lr = 1e-3)"
      ],
      "metadata": {
        "id": "HLLheIH472qK"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "for steps in range(10000):\n",
        "  xb, yb = get_batch('train')\n",
        "\n",
        "  loss, logits = n(xb, yb)\n",
        "\n",
        "  optimizer.zero_grad(set_to_none = True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if steps % 1000 == 0:\n",
        "    print(f'{loss.item() : .4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CyMU3RWZ8qSH",
        "outputId": "802aa9c3-0be9-40f5-b092-2524decc6dc9"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 4.7040\n",
            " 3.7031\n",
            " 3.1372\n",
            " 2.7768\n",
            " 2.5845\n",
            " 2.5105\n",
            " 2.5316\n",
            " 2.5048\n",
            " 2.4697\n",
            " 2.4839\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "indices = n.generate(idx = torch.zeros((1,1), dtype = torch.long), max_new_tokens = 100)\n",
        "print(decode(indices[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WC7j9PqZbOYR",
        "outputId": "40b5869a-8c08-4ed2-d540-dabd2528a250"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iyoteng h hasbe pave pirance\n",
            "Rie hicomyonthar's\n",
            "Plinseard ith henoure wounonthioneir thondy, y helti\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Summary**"
      ],
      "metadata": {
        "id": "U93vElf_Oq8t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bi-2xgFrPGt2",
        "outputId": "ac74ae87-1a5a-481c-8f25-0d3ce080f5b1"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7a03ac7989b0>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants\n",
        "\n",
        "batch_size = 32\n",
        "block_size = 8\n",
        "epochs = 10000\n",
        "eval_interval = 1000\n",
        "lr = 1e-3\n",
        "eval_epochs = 400"
      ],
      "metadata": {
        "id": "Cd3tzj2eOtp9"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(split):\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "  x = torch.stack([data[i: i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i + 1 : i + block_size + 1] for i in ix])\n",
        "  return x,y"
      ],
      "metadata": {
        "id": "_xe7pxPoPoZR"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss(model):\n",
        "  out = {}\n",
        "  model.eval()\n",
        "\n",
        "  for split in ['train', 'val']:\n",
        "    losses = torch.zeros(eval_epochs)\n",
        "    for k in range(eval_epochs):\n",
        "      X, Y = get_batch(split)\n",
        "      loss, logits = model(X, Y)\n",
        "      losses[k] = loss.item()\n",
        "    out[split] = losses.mean()\n",
        "  model.train()\n",
        "  return out"
      ],
      "metadata": {
        "id": "7zvsPqoI4Jtv"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramLanguageModel(nn.Module):\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets = None):\n",
        "    logits = self.token_embedding_table(idx)\n",
        "\n",
        "    if targets == None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B * T, C)\n",
        "      targets = targets.view(B * T)\n",
        "\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "    return (loss, logits)\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    for _ in range(max_new_tokens):\n",
        "      loss, logits = self(idx)\n",
        "      logits = logits[:, -1, :]\n",
        "      probs = F.softmax(logits, dim = -1)\n",
        "      idx_next = torch.multinomial(probs, num_samples = 1)\n",
        "      idx = torch.cat((idx, idx_next), dim = 1)\n",
        "    return idx"
      ],
      "metadata": {
        "id": "seaul1kYPq-4"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xb, yb = get_batch('train')\n",
        "print('inputs : ', xb.shape)\n",
        "print('targets : ', yb.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzXL-AhYQE81",
        "outputId": "e08051c3-c23c-4aef-8d3c-ae5ffd934de5"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs :  torch.Size([32, 8])\n",
            "targets :  torch.Size([32, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = BigramLanguageModel(vocab_size)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr = lr)\n",
        "\n",
        "for epoch in range(epochs + 1):\n",
        "\n",
        "  if epoch % eval_interval == 0:\n",
        "    losses = estimate_loss(model)\n",
        "    print(f\"epoch : {epoch:^5} | train_loss {losses['train'] : .4f} | val_loss {losses['val'] : .4f}\")\n",
        "\n",
        "  xb, yb = get_batch('train')\n",
        "\n",
        "  loss, logits = model(xb, yb)\n",
        "\n",
        "  optimizer.zero_grad(set_to_none = True)\n",
        "  loss.backward()\n",
        "  optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flLC5Sp04H2i",
        "outputId": "362b8f77-960b-450e-9df5-099d105bf69d"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch :   0   | train_loss  4.7126 | val_loss  4.7048\n",
            "epoch : 1000  | train_loss  3.7356 | val_loss  3.7344\n",
            "epoch : 2000  | train_loss  3.1374 | val_loss  3.1436\n",
            "epoch : 3000  | train_loss  2.8113 | val_loss  2.8177\n",
            "epoch : 4000  | train_loss  2.6411 | val_loss  2.6528\n",
            "epoch : 5000  | train_loss  2.5663 | val_loss  2.5732\n",
            "epoch : 6000  | train_loss  2.5284 | val_loss  2.5361\n",
            "epoch : 7000  | train_loss  2.5062 | val_loss  2.5111\n",
            "epoch : 8000  | train_loss  2.4748 | val_loss  2.4952\n",
            "epoch : 9000  | train_loss  2.4794 | val_loss  2.4922\n",
            "epoch : 10000 | train_loss  2.4658 | val_loss  2.4875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(model.generate(idx = torch.zeros((1,1), dtype = torch.long), max_new_tokens = 100)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qoXC0Bl97Fxb",
        "outputId": "f6175cd1-54e8-42cd-b672-47aeaf7f2f14"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "THAy wind s:\n",
            "Oh, 'dos t forandowig airimy mewhed hth\n",
            "TFo's lteache him, ukss y.\n",
            "\n",
            "wa ge-'s aryo rer n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above is just a simple bigram model which generates the text using only the last word, hence its generation is not proper and in the next, we could use a transformer model using multiple previous word embeds"
      ],
      "metadata": {
        "id": "AWpa_7L26iBb"
      }
    }
  ]
}